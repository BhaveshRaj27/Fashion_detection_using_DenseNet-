# -*- coding: utf-8 -*-
"""Clothes_multilabel_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJItnrqXITGzVmodnT7J0enmiBySXKcq
"""

import zipfile
from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow-gpu

drive.mount('/content/drive/')

zip_ref = zipfile.ZipFile('/content/drive/MyDrive/Copy of Copy of Image_1.zip')
zip_ref.extractall("/New_data")
zip_ref.close()

zip_ref = zipfile.ZipFile('/New_data/Image_1/Data.zip')
zip_ref.extractall("/New_data/Images")
zip_ref.close()

import os
import numpy as np
import pandas as pd
import cv2
import tensorflow as tf
import tensorflow.keras.backend as Back
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Dense, BatchNormalization
from sklearn.preprocessing import MultiLabelBinarizer
from tensorflow.keras.layers import concatenate, ReLU, Dropout,Flatten
from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D

# Function used to load data based on given data_format takes location of images(des_path), location of labels(csv_path) and size to resize the images as input 
# Return Images, coressponding labels and matching dictionary for labels
def data_loader(des_path,csv_path,size):
    csv=pd.read_csv(csv_path)
    csv= csv.set_index('imageId')
    a=os.listdir(des_path)
    X=[]
    Y=[]
    dic_y={}
    dic_x={}
    m=-1
    for i in a:

        b=des_path+'//'+str(i)
        j= csv.loc[int(i[:len(i)-4])]['labelId']
        c=j.replace('[','')
        d=c.replace(']','')
        g= d.split(',')
        new=[]
        for k in g:
            if k not in dic_y:
                m=m+1
                dic_y[k]=m
                dic_x[m]=k
            new.append(dic_y[k])
        Y.append(new)
        img=cv2.imread(des_path+'//'+i)
        img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img=cv2.resize(img,size)
        X.append(np.array(img))
    return(X,Y,dic_x)



# Dense_Net classes is created to bulid the model take inputs: input shape of model(input_shape), classes in the labels, Filters, total iteration to create Dense_Block, output activation function

class DenseNet():
    def __init__(self, input_shape, classes, Filters=32, total_iteration=[6,12,24,16], activation = 'sigmoid'):
        self.input_shape = input_shape
        self.classes = classes
        self.filters = Filters
        self.total_iteration = total_iteration
        self.activation = activation
  # Perform batch normalization, Apply Relu function and Do Convolution 
    def Relu_Conv_BatNorm(self,inp,stri=1, ker=1, filters=32):
        inp = BatchNormalization()(inp)
        inp = ReLU()(inp)
        out = Conv2D(filters, ker, strides=stri, padding='same')(inp)
        return out
    # Transition layer fuction us Relu_Conv_BatNorm function and then reduce the size(Downsampling) of output using AveagePooling
    def Trans_Layer(self,inp):
        inp = self.Relu_Conv_BatNorm(inp, filters=Back.int_shape(inp)[-1]//2)
        inp = Dropout(0.2)(inp)
        out = AvgPool2D(2, strides=2, padding='same')(inp)
        return out
    # Dense_Block function perform convolution while keeping size of feature map constant and use BottleNeck layer to stop feature map exploding
    def Dense_Block(self,inp, iteration):
        for _ in range(iteration):
            out = self.Relu_Conv_BatNorm(inp, filters=4*self.filters)
            out = self.Relu_Conv_BatNorm(out, ker=3, filters=self.filters)
            inp = concatenate([out, inp])
        return inp
    # Used to combine all the above layer use dropout and regularization to stop vanishing gradient problem.
    # Return the fully build densenet model
    def Run(self):
        inputs = Input(self.input_shape)
        out = Conv2D(64, 7 , strides=2, padding='same',kernel_regularizer=tf.keras.regularizers.l1_l2(0.01,0.001))(inputs)
        out = Dropout(0.1)(out)
        out = MaxPool2D(3, strides=2, padding='same')(out)
        
        for iterate in self.total_iteration:
            dense_out = self.Dense_Block(out, iterate)
            out = self.Trans_Layer(dense_out)
        out = GlobalAveragePooling2D()(dense_out)
        out = Dense(128,activation='relu')(out)
        out=Dropout(0.5)(out)
        out = Dense(64,activation='relu')(out)
        out = Dropout(0.5)(out)
        outputs = Dense(self.classes, activation=self.activation)(out)
        
        model = Model(inputs, outputs)
        #print(self.activation)
        return model

#Calling the DenseNet class and using run method to get the model
Dense_Net=DenseNet([224,224,3], 246)
model = Dense_Net.Run()
model.summary()

# Loading training data using data_loader
des_path='/New_data/Images/Data/train'
X_train, X_train_labels, matching_dict = data_loader(des_path,'/New_data/Image_1/Input/train_labels.csv',(224,224))

# Loading validation data using data_loader
des_path='/New_data/Images/Data/validation'
X_train_val, X_train_labels_val, matching_dict_val = data_loader(des_path,'/New_data/Image_1/Input/validation_labels.csv',(224,224))

#Converting labels into categorical form 
categorical=MultiLabelBinarizer().fit(X_train_labels)
# Using transfrom method to convert training and validation data
x_train_label=categorical.transform(X_train_labels)
x_val_label=categorical.transform(X_train_labels_val)

#Using adam optimizer and binary_crossentropy as loss fuction
adm=tf.keras.optimizers.Adam()
model.compile(loss='binary_crossentropy',optimizer=adm,metrics=['accuracy'])

# Providing checkpoint for model
checkpoint=tf.keras.callbacks.ModelCheckpoint('/fashion_model_23.h5',verbose=1,save_best_only=True)
#Appling Early stopping to stop overfitting
callbacks=[tf.keras.callbacks.EarlyStopping(patience=4,monitor='val_loss')]

r=model.fit(np.array(X_train),x_train_label, batch_size=100,verbose=1,epochs=30,validation_data=((np.array(X_train_val),x_val_label)),shuffle=True,callbacks=callbacks)

# Saving the model
model.save('/content/drive/MyDrive/fashion_model1.h5')

# Loading the model
model = tf.keras.models.load_model('/content/drive/MyDrive/fashion_model1.h5')

# creating dataframr to keep the test result and labels
df=pd.DataFrame({'ImageId':[],'Labels':[]})
des_path='/New_data/Images/Data/test'
# opening the test directory
a=os.listdir(des_path)
for i in a:
  # Reading and resize image as per the model
    img=cv2.imread(des_path+'//'+i)
    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img=cv2.resize(img,(224,224))
    #predicting output 
    ans=model.predict(np.array([img]))
    ans_list=[]
    # Adding the labels having probability of more than 0.5 
    for j in range(246):
      if ans[0][j]>0.5:
        # Matching the labels using maching_dict and appending in ans_list
        ans_list.append(matching_dict[j])
    # Adding imageId and prediction(ans_list) into dataframe
    df.loc[len(df.index)] = [i[:len(i)-4],ans_list]

# Converting the dataframe into csv and saving the csv
df.to_csv('/content/drive/MyDrive/PartA.csv')